

from keras.preprocessing.sequence import pad_sequences


import pandas as pd
import numpy as np
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, cohen_kappa_score
from keras.utils.vis_utils import plot_model
from sklearn.metrics import confusion_matrix
from keras.layers.normalization import BatchNormalization
from sklearn import metrics, cross_validation, preprocessing
# sklearn support
from sklearn import metrics, cross_validation, preprocessing
from sklearn.datasets.base import Bunch
from sklearn.model_selection import cross_val_score, cross_val_predict
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import ShuffleSplit, StratifiedKFold
from sklearn.tree import DecisionTreeRegressor
import matplotlib.pyplot as plt
import seaborn as sns

import keras
import keras_metrics
from keras import backend as K
import time

input_file = 'input.csv'

def load_data(test_split = 0.2):
    print ('Loading data...')
    df = pd.read_csv(input_file)
    df['sequence'] = df['sequence'].apply(lambda x: [int(e) for e in x.split()])
    df = df.reindex(np.random.permutation(df.index))

    train_size = int(len(df) * (1 - test_split))

    X_train = df['sequence'].values[:train_size]
    y_train = np.array(df['target'].values[:train_size])
    X_test = np.array(df['sequence'].values[train_size:])
    y_test = np.array(df['target'].values[train_size:])

    return pad_sequences(X_train), y_train, pad_sequences(X_test), y_test


def _plot_model():
    fig = plt.figure(figsize=(7,7))
    ax = fig.add_subplot(111)
    sns.heatmap(metrics.confusion_matrix(y_test, y_prediction),
            cmap="GnBu", square=True, ax=ax)
    ax.set_title('Heatmap: Confusion Matrix for \nKNN Classifier Model')
    ax.set_xlabel('Predicted ')
    ax.set_ylabel('Actual ')
    plt.show()
def scatter_y(true_y, predicted_y):
    """Scatter-plot the predicted vs true number of rings

    Plots:
       * predicted vs true number of rings
       * perfect agreement line
       * +2/-2 number dotted lines

    Returns the root mean square of the error
    """
    fig, ax = plt.subplots(figsize=(8, 8))
    ax.plot(true_y, predicted_y, '.k')

    ax.plot([0, 30], [0, 30], '--k')
    ax.plot([0, 30], [2, 32], ':k')
    ax.plot([2, 32], [0, 30], ':k')

    rms = (true_y - predicted_y).std()

    ax.text(25, 3,
            "Root Mean Square Error = %.2g" % rms,
            ha='right', va='bottom')

    ax.set_xlim(0, 30)
    ax.set_ylim(0, 30)

    ax.set_xlabel('True number of rings')
    ax.set_ylabel('Predicted number of rings')

    return rms

def get_metrices():
    print (metrics.classification_report(y_test, y_prediction))


X_train, y_train, X_test, y_test = load_data()

#training Step
model = DecisionTreeRegressor(max_depth=10)

model.fit(X_train,y_train)
predicted_test_y = model.predict(X_test)

predicted_train_y = model.predict(X_train)

print(predicted_train_y)
print(predicted_test_y)
#
scatter_y(y_train, predicted_train_y)
plt.title("Training data")
plt.show()
scatter_y(y_test, predicted_test_y)
plt.title("Test data");
plt.show()
# y_prediction = knc.predict(X_test)
# print (knc.score(X_test,y_test))
#
# get_metrices()
#
# _plot_model()


#comparing with other metrics
